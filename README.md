# Apache-Spark-End-To-End-Data-Engineering-Project-Apple-Data-Analysis
we used DataBricks to create multiple ETL pipelines using the Python API of Apache Spark i.e. PySpark.

We have used sources like CSV, Parquet, and Delta Table then used Factory Pattern to create the reader class. Factory Pattern is one of the most used Low-Level designs in Data Engineering pipelines that involve multiple sources.

Then we used PySpark DataFrame API and Spark SQL to write the business transformation logic. In the loader part, we have loaded data into two fashion one using DataLake and another by Data LakeHouse. 

Process:

1.https://community.cloud.databricks.com/

2. settings-Advanced-DBFS-on

   ![image](https://github.com/user-attachments/assets/49eb64bc-135f-4ec5-89d1-cc96c3bf35bb)

3.create a cluster
4.in catelog-DBFS-file store-tables

